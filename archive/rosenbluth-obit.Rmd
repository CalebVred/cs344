---
title: "Rosenbluth obit"
author: 
  - "Ken Arnold"
date: '2021-02-17'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE}
source("../slides-common.R")
slideSetup()
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
```

## [Obituary](https://www.nytimes.com/2021/02/09/science/arianna-wright-dead.html): Dr. Arianna Rosenbluth (1927-09-15 - 2020-12-28)

.pull-left[
```{r echo=FALSE, out.height="100%", fig.align='center'}
knitr::include_graphics("https://static01.nyt.com/images/2021/02/16/obituaries/05rosenbluth1/merlin_183269556_7d98de1d-baac-422a-8cf0-163383ac27f7-superJumbo.jpg")
```
]

.pull-right[
* PhD in Physics from Harvard at age 21
* Programmed the first implementation of the Metropolis algorithm, in machine code (1953)
]

---

## The Metropolis algorithm

Goal (for most ML applications): run a generative model backwards.
.pull-left[
* You have observations
* You propose a theory: a program that generates those observations
* But the program depends on unknown constants
* You need: good values of those constants
]

.pull-right[
* documents (collections of words)
* generate documents by sampling words from topics
* which words are in which topics?
* algorithm: Latent Dirichlet Allocation
]

(Imagine running your image classifier backwards: tell it "dog" and it should generate an image of a dog.)

---

> Instead of choosing configurations randomly, then weighting them with exp(−E/kT), we choose configurations with a probability exp(−E/kT) and weight them evenly.

<!-- ![](https://mbjoseph.github.io/posts/2018-12-25-animating-the-metropolis-algorithm/animating-the-metropolis-algorithm_files/figure-html5/create-gif.gif) -->

<https://mbjoseph.github.io/posts/2018-12-25-animating-the-metropolis-algorithm/>

