---
widget: blank
headless: true
design:
  # Choose how many columns the section has. Valid values: 1 or 2.
  columns: '1'

title: "Schedule"
weight: 5
---

```{r setup, include=FALSE}
library(tidyverse)
library(glue)
library(lubridate)
# devtools::install_github("kcarnold/daily")
library(daily)
```


## Schedule

```{r echo=FALSE, results='asis'}
add_prefix <- function(s, prefix) {
  str_replace_all(s, regex("^", multiline = TRUE), prefix)
}

cal_data <- 
  daily2cal(
  path = here::here("daily.txt"),
  end = lubridate::ymd("2021-03-03"),
) %>%
  pivot_longer(-c(date, week), names_to = "label", values_to = "contents") %>% 
  filter(
    !(str_to_lower(label) %in% c("plan", "log", "reflection", "next time")),
    !is.na(contents)) %>% 
  mutate(contents = str_trim(contents, side = "right")) %>% 
  filter(label != "Notes" | contents != "") %>% 
  mutate(contents = if_else(
    label %in% c("Quiz"),
    str_extract(contents, regex("^[^\\n]+$", multiline = TRUE)),
    contents
  ))
```

```{r echo=FALSE}
start <- lubridate::floor_date(min(cal_data$date), unit = "week")

cal_data %>% 
    mutate(topic_contents = glue("**{label}** {contents}")) %>% 
  group_by(date) %>% 
  summarize(contents = paste(topic_contents, collapse = "\n\n")) %>% 
  rowwise() %>% 
  mutate(html = markdown::markdownToHTML(text = contents, fragment.only = TRUE)) %>% 
  select(date, html) %>% 
  complete(date = seq(start, max(date), "day"), fill = list(html = "")) %>% 
  mutate(week  = as.numeric(floor(difftime(date, start, units = "weeks")))) %>% 
  html_calendar(show = 2:6) %>% 
  knitr::asis_output()
```


<style>
table ul {
  padding-left: 1rem;
}
</style>


```{r include=FALSE, eval=FALSE}
#' A version of purrr::accumulate that maintains the data type of the input.
#
# Needed because accumulate with dates coerces to `dbl`.

accumulate_custom <- function(x, f) {
  res <- x
  prev <- x[[1]]
  for(i in seq_along(x)) {
    if (i == 1) next;
    res[[i]] <- prev <- f(prev, x[[i]])
  }
  res
}

split_into_sections <- function(day_contents) {
  # If content starts with space or is only whitespace, it continues the previous
  # section. Otherwise, it's in the Private section.

  tibble(line = day_contents) %>% 
  filter(row_number() > 1) %>% 
  separate(line, into = c("section", "contents"), sep = "::", fill = "left") %>% 
  
  # starting with "  " continues sections.
  mutate(starts_with_whitespace = str_starts(contents, "  ")) %>% 
  
  # trim that starting "  ".
  mutate(contents = if_else(
    starts_with_whitespace, str_sub(contents, 3), contents)) %>% 
  
  # whitespace also means continue sections.
  mutate(should_continue_section =
    is.na(section) & 
      ((str_trim(contents) == "") | starts_with_whitespace)) %>% 
  group_by(section_idx = cumsum(!should_continue_section)) %>% 
  summarize(section = first(section), body = paste(contents, collapse = '\n')) %>% 
  replace_na(list(section = "Private")) %>% 
  select(-section_idx)
}

mask <- daily:::days2mask("MWF")
x <- here::here("daily.txt") %>% 
  read_lines() %>% 
  tibble(line = .) %>% 
  mutate(is_delim = str_starts(line, "==="), day_spec = cumsum(is_delim)) %>% 
  group_by(day_spec) %>% 
  summarize(daysep = first(line), rest = list(line)) %>% 
  filter(day_spec > 0) %>% 
  # remove `===` prefix
  mutate(daysep = str_sub(daysep, start = 4)) %>% 
  # mark skipped days (TODO handle)
  mutate(skip = str_detect(daysep, "skip")) %>% 
  # stop before end of calendar
  mutate(hit_end = cumsum(str_detect(daysep, "endofcal"))) %>% 
  filter(hit_end == 0) %>% 
  mutate(day = lubridate::mdy(daysep)) %>% 
  select(-daysep, -day_spec, -hit_end) %>% 
  # Fill in unspecified days
  mutate(day = accumulate_custom(day, function(prev_day, day) {
    if (is.na(day)) daily:::next_cal_day(prev_day, mask)
    else day
  })) %>% 
  rowwise() %>% 
  mutate(day_contents = list(split_into_sections(rest))) %>% 
  select(-rest) %>% 
  unnest(day_contents)
x
```




Overall, we will go through the first half of the Deep Learning for Coders book
at the pace that works for us, then see where we are. Then we will plan together
what remaining topics to discuss, such as:

* language processing (my specific area of interest), especially Transformers
* reinforcement learning
* unsupervised learning
* others as we identify them
