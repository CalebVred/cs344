{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Diagnostics\n",
    "\n",
    "Task: plot a confusion matrix, find images that were misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to read or modify the code in this section to successfully complete this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fastai code.\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# Set a seed for reproducibility.\n",
    "set_seed(0, reproducible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monkey-patch `plot_top_losses` because of a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_top_losses(self, k, largest=True, **kwargs):\n",
    "    losses,idx = self.top_losses(k, largest)\n",
    "    if not isinstance(self.inputs, tuple): self.inputs = (self.inputs,)\n",
    "    if isinstance(self.inputs[0], Tensor): inps = tuple(o[idx] for o in self.inputs)\n",
    "    else: inps = self.dl.create_batch(self.dl.before_batch([tuple(o[i] for o in self.inputs) for i in idx]))\n",
    "    b = inps + tuple(o[idx] for o in (self.targs if is_listy(self.targs) else (self.targs,)))\n",
    "    x,y,its = self.dl._pre_show_batch(b, max_n=k)\n",
    "    b_out = inps + tuple(o[idx] for o in (self.decoded if is_listy(self.decoded) else (self.decoded,)))\n",
    "    x1,y1,outs = self.dl._pre_show_batch(b_out, max_n=k)\n",
    "    if its is not None:\n",
    "        plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), self.preds[idx], losses,  **kwargs)\n",
    "ClassificationInterpretation.plot_top_losses = _plot_top_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = get_image_files(path).sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cat images have filenames that start with a capital letter.\n",
    "def is_cat(filename):\n",
    "    return filename[0].isupper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliberately corrupt some of the image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIP_PROB = 0.25\n",
    "correct_labels = [is_cat(path.name) for path in image_files]\n",
    "corrupted_labels = [\n",
    "    not correct_label if random.random() < FLIP_PROB else correct_label\n",
    "    for correct_label in correct_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many labels are still correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(\n",
    "    correct_label == corrupted_label\n",
    "    for correct_label, corrupted_label in zip(correct_labels, corrupted_labels)\n",
    ") / len(correct_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier on the (corrupted) labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = ImageDataLoaders.from_lists(\n",
    "    path=path, fnames=image_files, labels=corrupted_labels,\n",
    "    valid_pct=0.2,\n",
    "    seed=42,\n",
    "    item_tfms=Resize(224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(\n",
    "    dls=dataloaders,\n",
    "    arch=resnet18,\n",
    "    metrics=accuracy\n",
    ")\n",
    "learn.fine_tune(epochs=4)\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've given you a classifier (the `learn` object). It turns out that it was trained on a *corrupted* dataset where some of the labels were flipped, but let's pretend that we didn't know that. Could we figure out where the problems are by looking at the results of the classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Show one batch from each of the (corrupted) training and validation sets. (`dataloaders.train.show_batch()`)\n",
    "\n",
    "*Note: You may or may not actually see a mislabeled image here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "dataloaders.train.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "dataloaders.valid.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the *accuracy* and *error rate* of this classifier on the (corrupted) validation set (`accuracy(interp.preds, interp.targs)`). Check that this number matches the last accuracy figure reported while training above. Multiply this by the number of images in the validation set to give the actual number of misclassified images.\n",
    "\n",
    "*Hint*: you may need `WHATEVER.item()` to get a plain number instead of a `Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "print(\"Accuracy:\", accuracy(interp.preds, interp.targs).item())\n",
    "print(\"Error rate: \", error_rate(interp.preds, interp.targs).item())\n",
    "print(f\"Number of images incorrect: {round(error_rate(interp.preds, interp.targs).item() * corrupted_dataloaders.valid.n)} out of {corrupted_dataloaders.valid.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the confusion matrix on the (corrupted) validation set (see chapter 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the accuracy on the (corrupted) *training* set. (Since \"dataset 0\" is the training set and \"dataset 1\" is the validation set, we can use `interp_train = ClassificationInterpretation.from_learner(learn, ds_idx=0)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_train = ClassificationInterpretation.from_learner(learn, ds_idx=0)\n",
    "# your code here\n",
    "print(\"Accuracy:\", accuracy(interp_train.preds, interp_train.targs).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the top 12 losses in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the (corrupted) validation set, **how many dogs were misclassified as cats? Vice versa?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_incorrectly_labeled_cat = ...\n",
    "num_incorrectly_labeled_dog = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **If we had only looked at the accuracy on the (corrupted) training set, would we have *overestimated* or *underestimated* the validation set performance? By how much?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find some images in the validation set that were \"misclassified\" by looking at the top losses. **What does the classifier do with images that were mislabeled?** Explain what \"probability\" means in this output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
