---
title: "Tabular Data and Applications"
author: 
  - "Ken Arnold"
date: '2021-03-29'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: false
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source("../slides-common.R")
slideSetup()
```

## Recommender Systems

Think of a few experiences you've had with recommender systems.

* What sort of things were recommended to you?
* What were some signals that you gave to the system that the recommendation was good (or not)?
* What other data did the system have about you?
* What other data did the system have about the items it recommended to you?

---

class: center

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("images/yt-recsys-p1.png")
```

---

## RecSys Data

Draw an example of the *data table*(s) that YouTube might be using. (What are the columns?)

--

| timestamp | Viewer | Video | Watch time |
|----|---|---|---|---|
| 1616963421 | UC2nEn-yNA1BtdDNWziphPGA | WK_Nr4tUtl8 | 600 |
| 1616963422 | UCYO_jab_esuFRV4b17AJtAw | aircAruvnKk | 1153 |
| ...


* Tabular
* Categorical features with high *cardinality*
  * Lots of viewers!
  * Lots of videos!

---

## Core Idea: Similarity

* Find examples of people like you
* ... interacting with items like the ones being considered

So we need a way to measure *similarity* for both *users* and *items*

---

## Live Experimentation

> During development, we make extensive use of offline metrics (precision, recall, ranking loss, etc.) to guide iterative improvements to our system. However for the final determination of the effectiveness of an algorithm or model, **we rely on A/B testing via live experiments**. In a live experiment, we can measure subtle changes in click-through rate, watch time, and many other metrics that measure user engagement. This is important because **live A/B results are not always correlated with offline experiments**.

--

* They don't use likes, surveys, etc., but rather **watches**.
* Why? **Lots more data**! "allowing us to produce recommendations deep in the tail where explicit feedback is extremely sparse."

???

This is a very rich paper in terms of practical applications of machine learning in an organization.
I'll just highlight a few things.

---

class: center

```{r out.width="70%"}
knitr::include_graphics("images/yt-candidate-generation-no-serving.png")
```


---

## Exercise

On paper (or electronically), briefly explain collaborative filtering.

--

Share your explanation with a partner.

---

## Extension: Word Embeddings

"The meaning of a word is its use in the language" - Wittgenstein

"You know a word by the company it keeps"

```{r out.width="100%"}
knitr::include_graphics("images/slp3-skipgram.png")
```


---

```{r}
knitr::include_graphics("images/slp3-parallelogram.png")
```

.floating-source[Source: Jurafsky and Martin. [Speech and Language Processing 3rd ed](https://web.stanford.edu/~jurafsky/slp3/)]

---

.pull-left[
```{r out.width="100%"}
knitr::include_graphics("images/comparative_superlative.jpg")
```
]

.pull-right[
```{r out.width="100%"}
knitr::include_graphics("images/man_woman.jpg")
```
]

See also:
[Word embeddings quantify 100 years of gender and ethnic stereotypes](https://www.pnas.org/content/115/16/E3635) (Garg et al, PNAS 2018)

.floating-source[Source: [GloVe project](https://nlp.stanford.edu/projects/glove/)]

---

## Further Reading

* [Deep Neural Networks for YouTube Recommendations](https://research.google.com/pubs/pub45530.html), Covington et al, RecSys â€™16.
  * [Video of the presentation](https://www.youtube.com/watch?v=WK_Nr4tUtl8)
  * [Morning Paper summary](https://blog.acolyer.org/2016/09/19/deep-neural-networks-for-youtube-recommendations/)
* [Deep Learning Based Recommender System: A Survey and New Perspectives](https://doi.org/10.1145/3285029)
