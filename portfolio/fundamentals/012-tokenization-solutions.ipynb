{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "012-tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNS7mRS03a7VSFcbdUnYf/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcarnold/cs344/blob/main/portfolio/fundamentals/012_tokenization-solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jc8Qlh1TEgC"
      },
      "source": [
        "# `012` Tokenization\n",
        "\n",
        "Task: Convert text to numbers; interpret subword tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8_8RWp3TX-8"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUvTIxyWTdBF"
      },
      "source": [
        "We'll be using the HuggingFace Transformers library, which provides a (mostly) consistent interface to many different language models. We'll focus on the OpenAI GPT-2 model, famous for OpenAI's assertion that it was \"too dangerous\" to release in full.\n",
        "\n",
        "[Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for the model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWy--2nwhWPy",
        "outputId": "44b8e674-7e8b-4cf6-a1e9-1f8d62740382"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 21.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 36.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=d6b6fdd33995a4c16cbb13e95ff07cf348ad0cf8b10224cab129ff36417b6398\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKgPaDwhaN4"
      },
      "source": [
        "import torch\n",
        "from torch import tensor"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiNKbIh8hyDg"
      },
      "source": [
        "### Download and load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM5o_4w1hfyV"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True) # smaller version of GPT-2\n",
        "# Alternative to add_prefix_space is to use `is_split_into_words=True`\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-Z9_U0LUEVQ",
        "outputId": "1d639faf-5b56-4bb2-81e5-054ee086ef0a"
      },
      "source": [
        "print(f\"The model has {model.num_parameters():,d} parameters.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 81,912,576 parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOUiz_PsUZgS"
      },
      "source": [
        "## Task\n",
        "\n",
        "Consider the following phrase: \"In a shocking finding, scientists discovered a herd of unicorns living in\"\n",
        "\n",
        "**Getting familiar with tokens:**\n",
        "\n",
        "1. Use `tokenizer.tokenize` to convert the phrase into a list of tokens. What do you think the `Ġ` means?\n",
        "2. Use `tokenizer.convert_tokens_to_string` to convert the tokens back into a string.\n",
        "3. Use `tokenizer.encode` to convert the original phrase into token ids. (*Note: this is equivalent to `tokenize` followed by `convert_tokens_to_ids`*.)\n",
        "4. Use `tokenizer.decode` to convert the token ids back to the original phrase.\n",
        "\n",
        "**Applying what you learned:**\n",
        "\n",
        "5. Use `model.generate(tensor([input_ids]))` to generate a completion of this phrase. (Note that we needed to add `[]`s to give a \"batch\" dimension to the input.)\n",
        "6. Convert the result of `generate` into a readable form. (Recall the note in the previous step.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS7Z-DjoUiLK"
      },
      "source": [
        "phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\"\n",
        "#phrase = \"Next weekend I plan to\""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyq-5XWSUx_8",
        "outputId": "22efb7a8-37c5-46f0-e230-c3b8e5ad6bdc"
      },
      "source": [
        "# your code here\n",
        "tokens = tokenizer.tokenize(phrase)\n",
        "print(tokens)\n",
        "print(tokenizer.convert_tokens_to_string(tokens))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ĠIn', 'Ġa', 'Ġshocking', 'Ġfinding', ',', 'Ġscientists', 'Ġdiscovered', 'Ġa', 'Ġherd', 'Ġof', 'Ġunic', 'orns', 'Ġliving', 'Ġin']\n",
            " In a shocking finding, scientists discovered a herd of unicorns living in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkaoLSFMiHzb",
        "outputId": "18c6391e-a9aa-4d4c-dace-d49f8bbcba7a"
      },
      "source": [
        "# your code here\n",
        "input_ids = tokenizer.encode(phrase)\n",
        "input_ids"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[554,\n",
              " 257,\n",
              " 14702,\n",
              " 4917,\n",
              " 11,\n",
              " 5519,\n",
              " 5071,\n",
              " 257,\n",
              " 27638,\n",
              " 286,\n",
              " 28000,\n",
              " 19942,\n",
              " 2877,\n",
              " 287]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ncSRaBaZix8R",
        "outputId": "204670f9-d7c4-4856-c804-a038b77ccd1c"
      },
      "source": [
        "# your code here\n",
        "tokenizer.decode(input_ids)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' In a shocking finding, scientists discovered a herd of unicorns living in'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PZm3eIjjKCJ",
        "outputId": "1c4b1a63-de00-44f9-eee7-85714012dbc6"
      },
      "source": [
        "# your code here\n",
        "output_ids = model.generate(tensor([input_ids]))\n",
        "output_ids"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  554,   257, 14702,  4917,    11,  5519,  5071,   257, 27638,   286,\n",
              "         28000, 19942,  2877,   287,   257,  8222,   287,   262,  7840,   636]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2kKJ8rvijVez",
        "outputId": "386df167-0e88-45f1-b1a0-01beab1f0bc8"
      },
      "source": [
        "# your code here\n",
        "tokenizer.decode(output_ids[0])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' In a shocking finding, scientists discovered a herd of unicorns living in a forest in the northern part'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    }
  ]
}