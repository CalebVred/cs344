---
title: "CS 344: Artificial Intelligence"
semester: "Spring 2021"
time: "11:30am"
start: "02/03/2021"
end: "05/15/2021"
past_week: 11
days: "MWF"
---


===03/04/2021

Note:: Advising Day

===03/23/2021

Note:: Advising Day

===04/14/2021

Note:: Advising Day


===04/04/2021
Note:: Easter

===05/9/2021

Note:: Friday Class Schedule

===05/7/2021

Note:: Study Day




===02/03/2021
Topic:: Kickoff, [Teachable Machine](https://teachablemachine.withgoogle.com/train/image), Logistics
Read:: Syllabus
Resources:: [Day 1 slides](/slides/w1d1/w1d1-intro.html)

Log::
  * 15+ min: think-pair-share about intelligence in Scripture (Psalm 103 example)
  * 5 min: bare minimum logistics
  * 15 min: Teachable Machine in partners
  * 5 min: debrief Teachable Machine
  * 5 min: objectives and send-off

Reflection::
  * People got talking with each other.
  * People reflected on God's word and what it means for our class, in a way that
    will give persective to the rest of our time together.
  * People got experience with machine learning and data collection.

  It wasn't perfect, but it was the right way to start the semester.

Next time::
  * Shorter Scripture passage
  * Frame the Scripture TPS with an example?
  * Maybe have a backup device in case Teachable Machine doesn't work for someone?

===02/04/2021
Quiz:: Python review

  * Boolean function: does this string start with a capital letter?
  * func returning a string: strip everything after (and including) the last underscore
  * do this for every string in a list, without rewriting the logic
  * return the sum of the square of every element in a list

  ```
  x = 'grizzly', 'white', 'teddy'
  a,b,c = x
  print(c)
  ```

  ```
  class Thing:
    def __getitem__(self, x): return f"Item {x}"
    def __getattr__(self, x): return f"Attr {x}"

  # explain the result of each of the following lines.
  y = Thing
  y.abc
  y['abc']
  z = y()
  z.abc
  z['abc']
  z[abc]
  ```

  Suppose you have `def do_to_all(x, f): return [f(o) for o in x]`

  Fill in the blank:

  ```
  paths = ['a_b1', 'b_c2']
  stripped_paths = do_to_all(paths, _____)
  print(paths)
  ```

  So that you get ['a', 'b']. Explain why this works.



===
Topic:: Lab 0: Warm-up

Notes::
  <details><summary>Lab Logistics</summary>

  * Come to Maroon lab. Fill in computers as available, others stand around the sides of
    the room (at safe distance) for overview (then move to Gold lab)
  * People at Maroon lab computers: **reboot into Linux**

  </details>

===
Topic:: Lab 1 (Chapter 1)

Prep::
  * read [DL4C chapter 1](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb)
  * Watch [Lesson 1 Video](https://course.fast.ai/videos/?lesson=1)
  * Complete reading quiz


Next Time::
  * Simplify the questions.
  * Give some guidance on logistics (remind Lab 0: github, Maroon lab not VMs, full-screen trick)
  * Have students make some observations about individual images.


===
Topic:: Guest lecture: KVL

Due:: Reflection 1

===02/11/2021
Quiz:: Quiz 2

===
Topic:: Guest lecture: KVL

===
Topic:: Lab 1 recap ([slides](/slides/w2d1/w2d1-debrief.html), [code](https://nbviewer.jupyter.org/github/kcarnold/cs344/blob/main/src/Data_Loading_Code.ipynb))

Read:: [DL4C chapter 2](https://colab.research.google.com/github/fastai/fastbook/blob/master/02_production.ipynb)
    *note: ignore the implementation of `class DataLoaders`.*
Watch:: [Lesson 2 Video](https://course.fast.ai/videos/?lesson=2)
Quiz:: Reading Quiz 2

* Review lab 1. (peer feedback)
* Give feedback about weekly reviews
* Announce hw1

===
Topic:: Review, Intro to AI Ethics [slides](/slides/w2d2/w2d2-ethics.html)

Read::
  * [DL4C chapter 3](https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb) until "Topics in Data Ethics"
  * the **table of contents** of the [January 2021 Montreal AI Ethics Report](https://montrealethics.ai/wp-content/uploads/2021/01/State-of-AI-Ethics-Report-January-2021.pdf)

Due:: Discussion post about a topic that caught your eye (before class)

Due:: Reflection 2

===
Topic:: Lab 1 extension, homework work

* First peel-back-the-covers:
* Use DataBlocks API instead of default ImageDataLoaders
* show cat/dog batches
* compare performance with/without augmentation

Make sure this sets up well for hw1:
* What experiments do I hope students try for hw1
* How can I encourage meaningful comparisons? (Script to do a training run multiple times and collect evaluation outputs?)

Retrospective:

* Some students still struggled with git. Lots of wasted time there.
* Execution environment and collaboration is problematic.
* Unclear what exactly the goal was.

Next year: definitely do something easier than Yelp.


I'm wondering: should I move lab out of class? e.g., hold (unofficial) lab hours.

===
Topic:: Conceptual Review [Slides](/slides/w3d1/w3d1-concepts.html)

Plan::

  Logistics
  * Should I "override email prefs" for all Piazza announcements?
  * Shrink old calendar entries

  Core
  * Datasets: train/valid/test
  * Data loading: what problem does it solve
  * Confusion matrix; FP/FN / precision/recall
  * Batches
  * Data augmentation
    * analogies for overfitting
      * kids' puzzle
      * covid vaccines / antibodies / variants
      * background identification problem / dumbbell vs arm
    * what specific transforms can be used
  * Running experiments
    * Reproducibility
    * Variability
  * Previews:
    * loss
    * NN architectures

  Tools:

  * JSON newline-delimited
  * Jupyter Notebooks
    * execution order issues
  * github
    * stage / commit / push
    * nbdime?
    * integrations with Colab, Jupyter Lab
  * Collaboration
    * Teams screenshare
  * fastcore stuff, especially L

  If time: review Lab 1 + extensions


Read:: Finish reading [DL4C chapter 3](https://nbviewer.jupyter.org/github/fastai/fastbook/blob/master/03_ethics.ipynb); **Reading Quiz**

===
Topic:: Conceptual and Practical Review

Due:: Reflection 3

Discussion:: Reply in last week's Discussion

===
Topic:: Exploring Tensors

===
Topic:: Modeling Basics

Watch:: [Lesson 3 video](https://course.fast.ai/videos/?lesson=3)

Read:: [DL4C chapter 4](https://nbviewer.jupyter.org/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb) until "MNIST Loss Function" **Reading Quiz**

===
Topic:: Modeling Basics

Note:: Reflection delayed till next week

Note:: Add and upvote [application areas](https://calvincollege.sharepoint.com/sites/Section_77915/_layouts/15/Doc.aspx?sourcedoc={11c65f0d-7020-4c67-a7b2-93a5521628a6}&action=edit&wd=target%28_Collaboration%20Space%2FWeekly%20Notes.one%7Cf65e590f-924e-461a-ad2d-681cc376dd7c%2FApplication%20Areas%7C334a318a-1626-4e37-91fd-6bf983ef82d4%2F%29&wdorigin=703)

===3/4/2021
Quiz:: Technical Check-in

Released:: [Portfolio Repos](https://classroom.github.com/a/t9EfXnfw)

===
Read:: The rest of chapter 4

Watch:: The first hour of the [Lesson 4 video](https://course.fast.ai/videos/?lesson=4)


===
Watch:: The rest of the [Lesson 4 video](https://course.fast.ai/videos/?lesson=4) (masks postlude optional but interesting)

Read:: ch4 starting at "MNIST loss function", chapter 5 until "Model Interpretation"

Maybe the 3B1B YouTube videos?

===
Topic:: Chapter 5 review

===
Topic:: Lab

Spotlight:: [Taming Transformers](https://compvis.github.io/taming-transformers/)


===
Topic:: Lab

Read:: Rest of chapter 5, chapter 6

Watch:: [Lesson 6](https://course.fast.ai/videos/?lesson=6)

===
Topic:: Discussion and review

===
Topic:: Lab

Spotlight:: [Project Suggestions](/project)

Due:: Fundamentals 000-008 (suggested due date)

===

Lab:: Logistic Regression

Read:: Chapter 7 (active reading optional); Chapter 8; cumulative reading quiz


===
Topic:: Discussion on Facial Recognition Data

Spotlight:: [PapersWithCode Newsletter](https://paperswithcode.com/newsletter/)

Due:: Project Proposal Drafts

===
Topic:: Nonlinear Regression

===
Topic:: Collaborative Filtering

Read:: Through chapter 9

Watch:: [Lesson 7](https://course.fast.ai/videos/?lesson=7)

===
Topic:: Embeddings

===
Topic:: Collaborative Filtering and Embeddings in code

===
Topic:: Predictive Text ([slides](https://cs.calvin.edu/courses/cs/344/ka37/slides/2021-04-05%20Predictive%20Text%20Biases%20Writers.pdf))

===
Topic:: Language Processing 1

Read:: chapters 10 and 12 (skim the `xx` tokenization details; feel free to stop training early)

Watch:: [Lesson 8](https://course.fast.ai/videos/?lesson=8)

Notes:: [Jay Alammar](https://jalammar.github.io/) ([YouTube](https://www.youtube.com/channel/UCmOwsoHty5PrmE-3QhUBfPQ)) has made some nice visual explanations of language models.

===
Discussion:: Recommender Systems

Maybe today do the lab on embedding similarity? We have both movie similarity and word similarity.

===
Lab:: NLP

Watch:: Lecture 2 of [MIT 6.S191](http://introtodeeplearning.com/)

Try:: some [HuggingFace Transformers](https://huggingface.co/transformers/) notebooks, specifically: [overall functionality](https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/03-pipelines.ipynb) and [text generation](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb)

===04/13/2021
Opportunity:: [Human-Centered AI: Reliable, Safe and Trustworthy](https://iui.acm.org/2021/hcai_tutorial.html) (tutorial at Intelligent User Interfaces conferences)

===

advising day

===
Topic:: Generative Models ([slides](/slides/2021-04-16%20Generative%20Models.pdf))

Watch:: Lecture 4 of [MIT 6.S191](http://introtodeeplearning.com/) (and skim Lecture 3)

Try:: [GANLab](https://poloclub.github.io/ganlab/)

go through some of this? https://towardsdatascience.com/this-will-change-the-way-you-look-at-gans-9992af250454
but with a simpler density, like a Gaussian

https://github.com/ConorLazarou/medium/blob/master/12019/visualizing_gan_spiral/main.py

How can generative models be controlled?
- LMs: prefix, source/encoder
- GANs: noise vector
- VAEs: latent space
-- meaningful / disentangled directions in latent space

===
Topic:: Generative models continued

Topic:: Review

Review and finish the lecture from last time
Introduce DALL-E as a bridge to sequence models?
Review some Fundamentals notebooks or labs.


===
Topic:: Modern language models

Watch:: [fast.ai Transformer lecture](https://www.youtube.com/watch?v=AFkGPmU16QA&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=19) (from the [NLP course](https://www.fast.ai/2019/07/08/fastai-nlp/))

Read:: [Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)

other options:
Read [Attentional Interfaces](https://distill.pub/2016/augmented-rnns/#attentional-interfaces)
Watch [Attention Model Intuition](https://www.youtube.com/watch?v=SysgYptB198) and [details](https://www.youtube.com/watch?v=quoGRI-1l0A)

CLASS EXAMPLE: TRANSLATION EXAMPLE, WITH ATTENTION (question-and-answer)

===
Topic:: Lab (TBD)

Reference:: [Transformers from Scratch](http://peterbloem.nl/blog/transformers)


language generation:
- pull apart an encoder-decoder model
- fine-tune a few Transformers
- maybe look at https://github.com/karpathy/minGPT
- maybe do a prefix tuning example

CLIP: https://colab.research.google.com/drive/1bSQGtld1GYUweeJTKmm3k0npAe9IpVLn#scrollTo=htUBAqTAmM0k
https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/
Especially promising: Clip-Glass

* Text encoder
* Image encoder
* Matching
* TamingTransformers / Switching VAE generator -> Image encoder -> matching


===
Topic:: Human-AI Interaction (explainability, interpretability, etc.)

Watch:: [Stop doing "explainable" ML](https://www.youtube.com/watch?v=I0yrJz8uc5Q)

Skim:: [Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges](https://arxiv.org/abs/2103.11251)

Skim:: [Uncertainty as a Form of Transparency](https://arxiv.org/abs/2011.07586)

continuity: we're looking at visualizations of how models work; let's look at these.

https://github.com/hila-chefer/Transformer-Explainability / https://arxiv.org/abs/2012.09838v2
and https://arxiv.org/abs/2103.15679




===
Topic:: Fairness and Bias

Watch:: Lecture 6 of [MIT 6.S191](http://introtodeeplearning.com/)

Read:: [ACM Selects on Algorithmic Fairness](https://selects.acm.org/selections/why-algorithmic-fairness)

Read:: [The (Im)possibility of Fairness](https://cacm.acm.org/magazines/2021/4/251365-the-impossibility-of-fairness/fulltext)

Read:: <a href="https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1">Moving beyond “algorithmic bias is a data problem</a> (or this [thread](https://twitter.com/sarahookr/status/1361373527861915648))


===
Topic:: Reinforcement Learning

Watch:: Lecture 5 of [MIT 6.S191](http://introtodeeplearning.com/)

Watch:: [AlphaGo Documentary](https://www.youtube.com/watch?v=WXuK6gekU1Y) (optional)

Read:: [The Surprising Creativity of Digital Evolution](https://arxiv.org/abs/1803.03453v4)

===05/05/2021
Note:: Last ordinary class meeting

===05/13/2021
Topic:: Final Project Showcase

===endofcal
